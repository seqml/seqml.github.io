<!DOCTYPE html>
<html lang="en">
<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-67F1SX9G11"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-67F1SX9G11');
</script>

<meta charset="utf-8">
<meta name="generator" content="Hugo 0.66.0" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="../css/normalize.css">
<link rel="stylesheet" href="../css/skeleton.css">
<link rel="stylesheet" href="../css/custom.css">
<link rel="alternate" href="index.xml" type="application/rss+xml" title="ML for Sequence Data">
<link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
<title>Universal Trading for Order Execution with Oracle Policy Distillation</title>
</head>
<body>

<div class="container">

	<header role="banner">
		
	</header>


	<main role="main">
		<article itemscope itemtype="https://schema.org/BlogPosting">
            <h1 class="entry-title" itemprop="headline">Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble</h1>
			
			<section itemprop="entry-text">
				<br>
<!-- <img src="../images/opd/policy_distillation.png"  width="90%" height="40px"  alt="oracle_policy_distillation" /> -->
<p>ArXiv: <a target="_blank" and rel="noopener noreferrer" href="">TBD</a></p>
<p>Code: <a target="_blank" and rel="noopener noreferrer" href="TBD">TBD</a></p>
<p>Supplementary: <a target="_blank" and rel="noopener noreferrer" href="ijcai22_eppo_supplementary.pdf">PDF</a></p>
<h2 id="authors">Authors</h2>
<ul>
  <li>Zhengyu Yang (Shanghai Jiao Tong University) <a href="mailto:%20yzydestiny@sjtu.edu.cn">yzydestiny@sjtu.edu.cn</a></li>
  <li>Kan Ren (Microsoft Research) <a href="mailto:%20kan.ren@microsoft.com"> kan.ren@microsoft.com</a></li><li>Xufang Luo (Microsoft Research)&nbsp;<a href="mailto:%20xufluo@microsoft.com">xufluo@microsoft.com</a></li><li>Minghuan Liu (Shanghai Jiao Tong University)&nbsp;<a href="mailto:minghuanliu@sjtu.edu.cn">minghuanliu@sjtu.edu.cn</a></li>
  <li>Weiqing Liu (Microsoft Research) <a href="mailto:weiqing.liu@microsoft.com"> weiqing.liu@microsoft.com</a></li>
  <li>Jiang Bian (Microsoft Research)&nbsp;<a href="mailto:jiang.bian@microsoft.com">jiang.bian@microsoft.com</a></li>
  <li>Weinan Zhang (Shanghai Jiao Tong University) <a href="mailto:wnzhang@sjtu.edu.cn"> wnzhang@sjtu.edu.cn</a></li>
  <li>Dongsheng Li (Microsoft Research) <a href="mailto:%20dongsli@microsoft.com">dongsli@microsoft.com</a></li>
  </ul>
<h2 id="abstract">Abstract</h2>
<p>It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications. Take financial trading as an example, the market information is noisy yet imperfect and the macroeconomic regulation or other factors may shift between training and evaluation, thus it requires both generalization and high sample efficiency for resolving the task. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. To derive a robust and applicable RL algorithm, in this work, we design a simple but effective method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove that EPPO can increase exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at <a target="_blank" and rel="noopener noreferrer" href="https://seqml.github.io/eppo">https://seqml.github.io/eppo</a>.</p>

<!-- ## Code

Code will be released after the paper is accepted by the conference. -->
<h2 id="related-works">Related Works</h2>
<p>
  <a target="_blank" and="" rel="noopener noreferrer" href="https://papers.nips.cc/paper/2020/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf">The Diversified Ensemble Neural Network</a><br>
  <a target="_blank" and="" rel="noopener noreferrer" href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization Algorithms</a><br>
  <a target="_blank" and="" rel="noopener noreferrer" href="https://arxiv.org/pdf/1711.09874.pdf">Divide-and-Conquer Reinforcement Learning
</p>

			</section>
		</article>
	</main>


	

</div>


<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

 <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>




</body>
</html>

