<!DOCTYPE html>
<html lang="en">
<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-67F1SX9G11"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-67F1SX9G11');
</script>

<meta charset="utf-8">
<meta name="generator" content="Hugo 0.66.0" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="../css/normalize.css">
<link rel="stylesheet" href="../css/skeleton.css">
<link rel="stylesheet" href="../css/custom.css">
<link rel="alternate" href="index.xml" type="application/rss+xml" title="ML for Sequence Data">
<link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
<title>Bootstrapped Transformer for Offline Reinforcement Learning</title>
</head>
<body>

<div class="container">

<header role="banner">
</header>


<main role="main">
<article itemscope itemtype="https://schema.org/BlogPosting">
<h1 class="entry-title" itemprop="headline">Bootstrapped Transformer for Offline Reinforcement Learning</h1>
<section itemprop="entry-text">
<br>


<p>Arxiv: <a target="_blank" and rel="noopener noreferrer" href="https://arxiv.org/abs/2206.08569">https://arxiv.org/abs/2206.08569</a></p>
<p>Code: <a target="_blank" and rel="noopener noreferrer" href="https://github.com/microsoft/autorl-research/tree/main/bootorl">https://github.com/microsoft/autorl-research</a></p>
<!-- <p>Supplementary: <a target="_blank" and rel="noopener noreferrer" href="TBD">TBD</a></p> -->


<h2 id="authors">Authors</h2>
<ul>
<li>Kerong Wang (Shanghai Jiao Tong University) <a href="mailto: wangkerong@sjtu.edu.cn">wangkerong@sjtu.edu.cn</a></li>
<li>Hanye Zhao (Shanghai Jiao Tong University) <a href="mailto: fineartz@sjtu.edu.cn">fineartz@sjtu.edu.cn</a></li>
<li>Xufang Luo (Microsoft Research Asia) <a href="mailto: xufluo@microsoft.com"> xufluo@microsoft.com</a></li>
<li><a target="_blank" and rel="noopener noreferrer" href="http://www.saying.ren/">Kan Ren</a> (Microsoft Research Asia) <a href="mailto: kan.ren@microsoft.com"> kan.ren@microsoft.com</a></li>
<li>Weinan Zhang (Shanghai Jiao Tong University) <a href="mailto: wnzhang@sjtu.edu.cn">wnzhang@sjtu.edu.cn</a></li>
<li>Dongsheng Li (Microsoft Research Asia) <a href="mailto: kan.ren@microsoft.com"> dongsli@microsoft.com</a></li>
</ul>


<h2 id="abstract">Abstract</h2>
<p>Offline reinforcement learning (RL) aims at learning policies from previously collected static trajectory data without interacting with the real environment. Recent works provide a novel perspective by viewing offline RL as a generic sequence generation problem, adopting sequence models such as Transformer architecture to model distributions over trajectories, and repurposing beam search as a planning algorithm.</p>
<p>However, the training datasets utilized in general offline RL tasks are quite limited and often suffer from insufficient distribution coverage, which could be harmful to training sequence generation models yet has not drawn enough attention in the previous works. In this paper, we propose a novel algorithm named Bootstrapped Transformer (BooT), which incorporates the idea of bootstrapping and leverages the learned model to self-generate more offline data to further boost the sequence model training.</p>
<p>We conduct extensive experiments on two offline RL benchmarks and demonstrate that our model can largely remedy the existing offline RL training limitations and beat other strong baseline methods. We also analyze the generated pseudo data and the revealed characteristics may shed some light on offline RL training.</p>


<h2 id="algorithm">Algorithm Overview</h2>
<p>The overall idea of our BooT algorithm is to utilize self-generated trajectories as auxiliary data to further train the model, which is the general idea of bootstrapping. We divide BooT into two main parts, (i) generating trajectories with the learned sequence model and (ii) using the generated trajectories to augment the original offline dataset for bootstrapping the sequence model learning.</p>
<img src="trajectory_generation.png" alt="Trajectory generation demonstration" style="width: 90%; display: block; margin-left: auto; margin-right: auto;">
<p>Basically, for each batch of input trajectories, we first train the model with original trajectories. We then generate new trajectories based on the original trajectories, filter them with their quality, and finally train the model again with the generated trajectories. The trajectory generation procedure is demonstrated in the figure above.</p>


<h2 id="experiment">Experiment Results</h2>
<p>We evaluate the performance of BooT algorithm on the dataset of continuous control tasks from the D4RL offline dataset. We compared BooT to various baselines as shown in the table.</p>
<img src="experiment_result_gym.png" alt="Experiment results on Gym domain of D4RL offline dataset" style="width: 80%; display: block; margin-left: auto; margin-right: auto;">


<h2 id="analysis">Further Analysis of Bootstrapping</h2>
<p>To better understand how BooT helps improve the performance, we visualize the distribution of transitions in the (i) original data, (ii) from teacher-forcing generation, and (iii) from autoregressive generation in BooT. We reduce them to 2-dimension via t-SNE altogether, and plot them separately.</p>
<img src="generation_visualization.png" alt="Experiment results on Gym domain of D4RL offline dataset" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
<p>The overall results demonstrate that generated trajectories from BooT expand the data coverage while still keeping consistency with the underlying MDP of the corresponding RL task, thus resulting in better performance compared to the baselines.</p>


<!-- <h2 id="code">Code</h2>
Code will be released after the paper is accepted by the conference. -->


<h2 id="related-works">Related Works</h2>
<p>
<a target="_blank" and rel="noopener noreferrer" href="https://proceedings.neurips.cc/paper/2021/file/099fe6b0b444c23836c4a5d07346082b-Paper.pdf">Offline Reinforcement Learning as One Big Sequence Modeling Problem</a><br>
</p>


<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

 <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

 <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>

</body>
</html>